---
title: "Lois univariées usuelles"
output: html_notebook
---

## 1. Introduction

Dans ce notebook nous allons étudier quelques lois usuelles en statistique, illustrer leurs densités, et simuler des données distribuées selon ces lois, et dessiner leurs densités. Nous allons illustrer quelques convergences en loi bien connues dans ce cadre, et étudier l'estimateur au maximum de vraisemblance

## 2. Densités et histogrammes

Dans cette partie nous voulons:

- dessiner la densité pour plusieurs valeurs des paramètres (on les choisira pour faire un joli graphe)
- simuler des observation selon cette loi, et afficher l'histogramme associé, en même temps que la densité

pour les lois listées dans les sous-sections en dessous. 
On utilisera les fonctions proposées par `R` qui permettent de simuler, et de calculer les densités: on utilisera par 
exemple les fonctions `rpois()`, `rexp()`, `rnorm()`, `runif()`, `rbeta()`, `rgamma()`, `rchisq()`, `rcauchy()`, 
`rstudent()` pour la simulation, selon les différentes lois.

### 2.1. Loi normale $\mathcal N(\mu, \sigma^2)$

On utilise la fonction `dnorm`, qui permet de calculer la valeur de la densité normale

```{r}
x = seq(-5, 5, by=1/100)

plot(x, dnorm(x, mean=0, sd=1), type='l', col=1, lwd=3, ylab='densite', ylim=c(0, 1.5))
lines(x, dnorm(x, mean=0, sd=2), col=2, lwd=3)
lines(x, dnorm(x, mean=0, sd=0.5), col=3, lwd=3)
title('Densite normale')
legend("topright", legend=c('mean=0, sd=1', 'mean=0, sd=2', 'mean=0, sd=0.5'), 
       col=c(1, 2, 3), lwd=c(3, 3, 3))

plot(x, dnorm(x, mean=-1, sd=1), type='l', col=1, lwd=3, ylab='densite', ylim=c(0, 1.5))
lines(x, dnorm(x, mean=-1, sd=2), col=2, lwd=3)
lines(x, dnorm(x, mean=-1, sd=0.5), col=3, lwd=3)
title('Densite normale')
legend("topright", legend=c('mean=-1, sd=1', 'mean=-1, sd=2', 'mean=-1, sd=0.5'), 
       col=c(1, 2, 3), lwd=c(3, 3, 3))
```

```{r}
n = 200
m = 0

par(mfrow=c(1, 2))
s = 0.5
X = rnorm(n, mean=m, sd=s)
zeros = rep(0, n)
plot(X, zeros, pch='|', ylim=c(0, 0.9), xlim=c(-3, 3), ylab='Densite')
x = seq(-3, 3, by = 1/100)
lines(x, dnorm(x, mean=m, sd=s), lwd=3)
title()
legend('topright', c('Densite de N(0, 1/4)', 'observations'), 
       pch=c('-', '|'), lwd=c(3, 1))

s = 1
X = rnorm(n, mean=m, sd=s)
zeros = rep(0, n)
plot(X, zeros, pch='|', ylim=c(0, 0.9), xlim=c(-3, 3), ylab='Densite')
x = seq(-3, 3, by = 1/100)
lines(x, dnorm(x, mean=m, sd=s), lwd=3)
title()
legend('topright', c('Densite de N(0, 1)', 'observations'), 
       pch=c('-', '|'), lwd=c(3, 1))
```

```{r}
n = 2000
X = rnorm(n, mean=m, sd=s)
hist(X, breaks=20, freq=FALSE)
x = seq(min(X), max(X), by=1 / 100)
lines(x, dnorm(x, mean=m, sd=s), lwd=3)
```

### 2.2. Loi exponentielle $\mathcal E(\lambda)$


```{r}
rates = c(0.5, 1, 2)
x = seq(0, 5, by=1/100)

plot(x, dexp(x, rate=rates[1]), type='l', col=1, lwd=3, ylab='densite', ylim=c(0, 1.5))
lines(x, dexp(x, rate=rates[2]), col=2, lwd=3)
lines(x, dexp(x, rate=rates[3]), col=3, lwd=3)
title('Densites exponentielle')
legend("topright", legend=c('rate=0.5', 'rate=1', 'rate=2'), 
       col=c(1, 2, 3), lwd=c(3, 3, 3))
```

```{r}
n = 200
par(mfrow=c(1, 3))
rates = c(0.1, 1, 5)

x = seq(0, 5, by=1/100)
X = rexp(n, rate=rates[1])
zeros = rep(0, n)
plot(X, zeros, pch='|', ylab='Densite', xlim=c(0, 5), ylim=c(0, 1))
lines(x, dexp(x, rate=rates[1]), type='l', col=1, lwd=3)
title('Exponentielle rate=0.1')
X = rexp(n, rate=rates[2])
zeros = rep(0, n)
plot(X, zeros, pch='|', ylab='Densite', xlim=c(0, 5), ylim=c(0, 1))
lines(x, dexp(x, rate=rates[2]), type='l', col=1, lwd=3)
title('Exponentielle rate=1')
X = rexp(n, rate=rates[3])
zeros = rep(0, n)
plot(X, zeros, pch='|', ylab='Densite', xlim=c(0, 5), ylim=c(0, 1))
lines(x, dexp(x, rate=rates[3]), type='l', col=1, lwd=3)
title('Exponentielle rate=5')
```
```{r}
n = 1000

par(mfrow = c(2, 2))

rate = 0.1
X = rexp(n, rate=rate)
hist(X, freq=FALSE, main='')
x = seq(0, max(X), by=1/100)
lines(x, dexp(x, rate=rate), col='blue', lwd=3)

title('Exponentielle avec rate=0.1')

rate = 1
X = rexp(n, rate=rate)
hist(X, freq=FALSE, main='')
x = seq(0, max(X), by=1/100)
lines(x, dexp(x, rate=rate), col='blue', lwd=3)
title('Exponentielle avec rate=1')

rate = 5
X = rexp(n, rate=rate)
hist(X, freq=FALSE, main='')
x = seq(0, max(X), by=1/100)
lines(x, dexp(x, rate=rate), col='blue', lwd=3)
title('Exponentielle avec rate=5')

rate = 10
X = rexp(n, rate=rate)
hist(X, freq=FALSE, main='')
x = seq(0, max(X), by=1/100)
lines(x, dexp(x, rate=rate), col='blue', lwd=3)
title('Exponentielle avec rate=10')
```

### 2.3. Loi de Poisson $\mathcal P(\lambda)$


```{r}

x = 1:15
lambdas = c(0.5, 3, 5, 10)
plot(x, dpois(x, lambda=lambdas[1]),col=1, ylab='densite', ylim=c(0, 0.4), lwd=2)
points(x, dpois(x, lambda=lambdas[2]), col=2, lwd=2)
points(x, dpois(x, lambda=lambdas[3]), col=3, lwd=2)
points(x, dpois(x, lambda=lambdas[4]), col=4, lwd=2)
title('Densites de Poisson')
legend("topright", legend=c('lambda=0.5', 'lambda=3', 'lambda=5', 'lambda=10'), 
       col=c(1, 2, 3, 4), lwd=c(2, 2, 2, 2), pch='o')

```

```{r}
n = 200

lambdas = c(0.5, 3, 5, 10)

par(mfrow = c(2, 2))
X = rpois(n, lambda=lambdas[1])
plot(prop.table(table(X)))
x = 0:max(X)
points(x, dpois(x, lambda=lambdas[1]), lwd=3)
title('Poisson avec lambda=0.5')

X = rpois(n, lambda=lambdas[2])
plot(prop.table(table(X)))
x = 0:max(X)
points(x, dpois(x, lambda=lambdas[2]), lwd=3)
title('Poisson avec lambda=3')

X = rpois(n, lambda=lambdas[3])
plot(prop.table(table(X)))
x = 0:max(X)
points(x, dpois(x, lambda=lambdas[3]), lwd=3)
title('Poisson avec lambda=5')

X = rpois(n, lambda=lambdas[4])
plot(prop.table(table(X)))
x = 0:max(X)
points(x, dpois(x, lambda=lambdas[4]), lwd=3)
title('Poisson avec lambda=10')

# freq = FALSE, breaks=max(X) + 1)
```

### 2.4. Loi uniforme $\mathcal U(a, b)$


### 2.5. Loi Beta $\beta(a, b)$


### 2.6. Loi du $\chi_2(n)$


### 2.7. Loi de Cauchy


### 2.8. Loi de Student


## 3. Quelques approximations en loi classiques 

Dans cette section nous illustrons quelques convergences en loi vues en cours.

### 3.1. Approximation en loi binomiale Poisson

On sait d'après le cours que si $X_n$ est une suite de variables aléatoire de loi binomiale $\mathcal B(n, p_n)$, telle que $n p_n \rightarrow \lambda$, alors $X_n$ converge en loi vers une loi $\mathcal P(\lambda)$ (Poisson d'intensité $\lambda$).
Illustrer cette convergence en loi par simulation, en comparant l'histogramme de simulations de loi $\mathcal B(n, p_n)$ 
avec celle d'une densité de loi $\mathcal P(\lambda)$.

```{r}
lambda = 3.
# Le nombre de simulations de binomiales
n_samples = 500
# Les valeurs de n dans les binomiales
sizes = c(10, 50, 500, 1000)
ylim = c(0, 0.3)

par(mfrow = c(2, 2))
for (size in sizes) {
  p = lambda / size
  X = rbinom(n=n_samples, p, size=size)
  plot(prop.table(table(X)), ylab = 'Density', ylim=ylim)
  title(paste("B(n, lambda / n) avec n=", toString(size)))
  x = 0:max(X)
  points(x, dpois(x, lambda=lambda), lwd=3, col='red')
}
```

### 3.2. Illustration du TCL

Choisissez une loi (une parmi celles listées au dessus) avec une variance finie. Illustrez le théorème centrale limite pour cette loi, en simulant des échantillons $X_1^{(1)}, \ldots, X_n^{(1)}, \ldots, X_1^{(B)}, \ldots, X_n^{(B)}$ avec $n$ et $B$ assez grand, simulés selon cette loi, et dessinez l'histogramme des valeurs
$$
\sqrt n (\bar X_n^{(b)} - \mathbb E(X_1))
$$
pour $b=1, \ldots, B$ avec $n$ et $B$ assez grands où $\bar X_n^{(b)} = \frac 1n \sum_{i=1}^n X_i^{(b)}$.

```{r}
B = 200
n_sampless = c(10, 50, 100, 5000)
xlim = c(-2, 2)
ylim = c(0, 1)

x = seq(-3, 3, by=1/100)

# On va considerer les echantillons de loi uniforme sur [a, b] avec a = -1 et b = 1
a = -1
b = 1
# L'esperance est 0 donc
m = 0
# Et la variance est (b - a)^2 / 12 = 1/3
v = 1 / 3

par(mfrow = c(2, 2))
for (n_samples in n_sampless) {
  X = matrix(runif(n=n_samples * B, min=a, max=b), nrow=n_samples, ncol=B)
  hist(sqrt(n_samples) * (colMeans(X) - m), breaks=15, freq=FALSE, main='', xlim=xlim, ylim=ylim)
  lines(x, dnorm(x, mean=m, sd=sqrt(v)), lwd=3, col="blue")
  title(paste("n=", toString(n_samples)))
}
  
```

```{r}
B = 200
n_sampless = c(10, 50, 100, 5000)
xlim = c(-2, 2)
ylim = c(0, 1)

x = seq(-3, 3, by=1/100)

# On va considerer les echantillons de loi exponentielle d'intensite 2
rate = 2
# L'esperance est 
m = 1 / 2
# Et la variance
v = 1 / 4

par(mfrow = c(2, 2))
for (n_samples in n_sampless) {
  X = matrix(rexp(n=n_samples * B, rate=rate), nrow=n_samples, ncol=B)
  hist(sqrt(n_samples) * (colMeans(X) - m), breaks=15, freq=FALSE, main='', xlim=xlim, ylim=ylim)
  lines(x, dnorm(x, mean=0, sd=sqrt(v)), lwd=3, col="blue")
  title(paste("n=", toString(n_samples)))
}
```

```{r}
B = 200
n_sampless = c(10, 50, 100, 5000)
xlim = c(-5, 5)
ylim = c(0, 0.4)

x = seq(-5, 5, by=1/100)

# On va considerer des echantillons de loi de Poisson d'intensite lambda
lambda = 2
# L'esperance est 
m = lambda
# Et la variance
v = lambda

par(mfrow = c(2, 2))
for (n_samples in n_sampless) {
  X = matrix(rpois(n=n_samples * B, lambda=lambda), nrow=n_samples, ncol=B)
  hist(sqrt(n_samples) * (colMeans(X) - m), breaks=15, freq=FALSE, main='', xlim=xlim, ylim=ylim)
  lines(x, dnorm(x, mean=0, sd=sqrt(v)), lwd=3, col="blue")
  title(paste("n=", toString(n_samples)))
}
```


### 3.3. Approximation Student vers la loi normale

La loi de Student $t(n)$ à $n$ degrés de libertés converge en loi vers la loi normale $N(0, 1)$. 
Illustrer cela de la même façon que pour la question~3.1.


## 4. Maximum de vraisemblance

Dans cette section nous voulons illustrer le principe de maximum de vraisemblance dans des cas simples, ou l'estimateur est explicite, puis traiter des exemples ou l'estimateur n'est pas explicite.


### 4.1 Maximum de vraisemblance pour la loi de Poisson

Choissons la loi de Poisson. On cherche à maximiser la vraisemblance donnée par
$$
L_\lambda(x_1, \ldots, x_n) = \prod_{i=1}^n e^{-\lambda} \frac{\lambda^{x_i}}{x_i !},
$$
ou de facon équivalente, à minimiser la moins log-vraisemblance
$$
- \ell_\lambda(x_1, \ldots, x_n) = n \lambda - (\log \lambda) \sum_{i=1}^n x_i + \sum_{i=1}^n \log(x_i!)
$$

donc quand on maximise par rapport a $\lambda$, on obtient
$$
\hat \lambda_n = \frac 1n \sum_{i=1}^n X_i
$$
Dessisons la log vraisemblance

```{r}
lambda0 = 3
n = 100

X = rpois(n=n, lambda=lambda0)
lambdas = seq(1/100, 5, by=1/100)

log_vrai <- function(lambdas) {
  return(n * lambdas - log(lambdas) * sum(X) + sum(log(factorial(X))))
}
plot(lambdas, log_vrai(lambdas), type='l')
```

On observe qu'effectivement la log-vraisemblance negative est bien minimale en une valeur proche de l'intensité

### 4.1. Loi exponentielle

Faisons la même chose pour la loi exponentielle. Simulez un échantillon de loi $\mathcal E(\lambda)$.
Illustrer la fonction de moins log-vraisemblance pour le modèle exponentiel sur cet échantillon.
Rappelez la valeur de l'estimateur au maximum de vraisemblance pour ce modèle et affichez cette valeur sur cette illustration.

### 4.2. Loi Gamma

Simulez un échantillon de loi $\Gamma(a, \lambda)$. L'estimateur au maximum de vraisemblance des deux paramètres de cette loi n'est pas explicite: on peut alors l'obtenir de façon numérique.
Ecrivez une fonction qui calcule l'EMV de $a$ et de $\lambda$ à l'aide de la fonction `optim`: lisez sa documentation et utilisez la pour trouver l'EMV.

On rappelle que la densité de la loi $\Gamma(a, \lambda)$ est donnée par
$$
f_{a, \lambda}(x) = \frac{1}{\Gamma(a)} x^{a-1} \lambda^a e^{-\lambda x} \mathbf 1_{x \geq 0}
$$
```{r}
a0 = 4.2
lambda0 = 1.5
x = seq(0, 10, by=1/100)
y = dgamma(x, shape=a0, rate=lambda0)
plot(x, y, type='l')
```

```{r}
n = 2000
# On simule des donnees de loi Gamma
X = rgamma(n, shape=a0, rate=lambda0)

# Cette fonction calcul la moins log-vraisemblance de la loi Gamma
# X est le jeu de donnees
# x est un vecteur a deux coordonnees qui contient a et lambda
log_lik = function(x) {
  return(- sum(log(dgamma(X, shape=x[1], rate=x[2]))))
}

# On initialise d'une facon ou d'une autre
shape_init = 1 / mean(X)
rate_init = mean(X)

# On cherche un minimiseur de cette fonction
# On utilise le solver "L-BFGS-B" (quasi-Newton), on indique une borne 
# inferieure pour les paramètres a et lambda (ils doivent être positifs)
res = optim(c(shape_init, rate_init), log_lik, method="L-BFGS-B", 
            lower=1e-10)

res$par
```

On observe que l'on retrouve de bonnes estimations des vrais paramètres choisis (égaux à 4.2 et 1.5)
